frauddetection<class 'pandas.core.frame.DataFrame'>
RangeIndex: 284807 entries, 0 to 284806
Data columns (total 31 columns):
 #   Column  Non-Null Count   Dtype  
---  ------  --------------   -----  
 0   Time    284807 non-null  float64
 1   V1      284807 non-null  float64
 2   V2      284807 non-null  float64
 3   V3      284807 non-null  float64
 4   V4      284807 non-null  float64
 5   V5      284807 non-null  float64
 6   V6      284807 non-null  float64
 7   V7      284807 non-null  float64
 8   V8      284807 non-null  float64
 9   V9      284807 non-null  float64
 10  V10     284807 non-null  float64
 11  V11     284807 non-null  float64
 12  V12     284807 non-null  float64
 13  V13     284807 non-null  float64
 14  V14     284807 non-null  float64
 15  V15     284807 non-null  float64
 16  V16     284807 non-null  float64
 17  V17     284807 non-null  float64
 18  V18     284807 non-null  float64
 19  V19     284807 non-null  float64
 20  V20     284807 non-null  float64
 21  V21     284807 non-null  float64
 22  V22     284807 non-null  float64
 23  V23     284807 non-null  float64
 24  V24     284807 non-null  float64
 25  V25     284807 non-null  float64
 26  V26     284807 non-null  float64
 27  V27     284807 non-null  float64
 28  V28     284807 non-null  float64
 29  Amount  284807 non-null  float64
 30  Class   284807 non-null  int64
dtypes: float64(30), int64(1)
memory usage: 67.4 MB
None
                Time            V1            V2            V3            V4  ...           V26           V27           V28         Amount          Class
count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  ...  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000  284807.000000
mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15  ...  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619       0.001727
std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00  ...  4.822270e-01  4.036325e-01  3.300833e-01     250.120109       0.041527
min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00  ... -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000       0.000000
25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01  ... -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000       0.000000
50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02  ... -5.213911e-02  1.342146e-03  1.124383e-02      22.000000       0.000000
75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01  ...  2.409522e-01  9.104512e-02  7.827995e-02      77.165000       0.000000
max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01  ...  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000       1.000000

[8 rows x 31 columns]
   Time        V1        V2        V3        V4        V5        V6        V7  ...       V23       V24       V25       V26       V27       V28  Amount  Class
0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599  ... -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0
1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803  ...  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69      0
2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461  ...  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0
3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609  ... -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50      0
4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941  ... -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99      0

[5 rows x 31 columns]
Time      0
V1        0
V2        0
V3        0
V4        0
V5        0
V6        0
V7        0
V8        0
V9        0
V10       0
V11       0
V12       0
V13       0
V14       0
V15       0
V16       0
V17       0
V18       0
V19       0
V20       0
V21       0
V22       0
V23       0
V24       0
V25       0
V26       0
V27       0
V28       0
Amount    0
Class     0
dtype: int64
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56864
           1       0.89      0.84      0.86        98

    accuracy                           1.00     56962
   macro avg       0.95      0.92      0.93     56962
weighted avg       1.00      1.00      1.00     56962

Confusion Matrix:
[[56854    10]
 [   16    82]]



//Output creditcardScript.py

Epoch 1/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.9632 - loss: 0.0919 - val_accuracy: 0.9991 - val_loss: 0.0104
Epoch 2/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9946 - loss: 0.0180 - val_accuracy: 0.9998 - val_loss: 0.0052
Epoch 3/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9963 - loss: 0.0131 - val_accuracy: 0.9999 - val_loss: 0.0040
Epoch 4/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9965 - loss: 0.0118 - val_accuracy: 0.9987 - val_loss: 0.0056
Epoch 5/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9971 - loss: 0.0104 - val_accuracy: 1.0000 - val_loss: 0.0033
Epoch 6/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9975 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 0.0027
Epoch 7/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9976 - loss: 0.0091 - val_accuracy: 1.0000 - val_loss: 0.0040
Epoch 8/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 20s 2ms/step - accuracy: 0.9977 - loss: 0.0088 - val_accuracy: 0.9999 - val_loss: 0.0024
Epoch 9/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 2ms/step - accuracy: 0.9978 - loss: 0.0081 - val_accuracy: 1.0000 - val_loss: 0.0019
Epoch 10/10
12795/12795 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.9980 - loss: 0.0079 - val_accuracy: 0.9994 - val_loss: 0.0041
1781/1781 ━━━━━━━━━━━━━━━━━━━━ 1s 707us/step 
Classification Report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00     56864
           1       0.67      0.85      0.75        98

    accuracy                           1.00     56962
   macro avg       0.84      0.92      0.88     56962
weighted avg       1.00      1.00      1.00     56962

Confusion Matrix:
[[56824    40]
 [   15    83]]





//output ecommercefraudScript.py

Epoch 1/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 10s 1ms/step - accuracy: 0.5085 - loss: 7830660.5000 - val_accuracy: 0.0017 - val_loss: 0.9454
Epoch 2/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5553 - loss: 1.0688 - val_accuracy: 2.2812e-04 - val_loss: 0.8120
Epoch 3/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5551 - loss: 0.7008 - val_accuracy: 0.0000e+00 - val_loss: 0.8241
Epoch 4/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5568 - loss: 0.8140 - val_accuracy: 0.0000e+00 - val_loss: 0.8106
Epoch 5/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5578 - loss: 0.6877 - val_accuracy: 0.0000e+00 - val_loss: 0.7961
Epoch 6/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5577 - loss: 115.8693 - val_accuracy: 0.0000e+00 - val_loss: 0.8082
Epoch 7/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 11s 2ms/step - accuracy: 0.5567 - loss: 0.6871 - val_accuracy: 0.0000e+00 - val_loss: 0.8057
Epoch 8/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 10s 1ms/step - accuracy: 0.5565 - loss: 0.6867 - val_accuracy: 0.0000e+00 - val_loss: 0.8151
Epoch 9/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5549 - loss: 0.6872 - val_accuracy: 0.0000e+00 - val_loss: 0.8363
Epoch 10/10
6165/6165 ━━━━━━━━━━━━━━━━━━━━ 9s 1ms/step - accuracy: 0.5568 - loss: 2.4443 - val_accuracy: 0.0000e+00 - val_loss: 0.8082
945/945 ━━━━━━━━━━━━━━━━━━━━ 1s 878us/step
Classification Report:
              precision    recall  f1-score   support

           0       0.91      1.00      0.95     27373
           1       0.00      0.00      0.00      2850

    accuracy                           0.91     30223
   macro avg       0.45      0.50      0.48     30223
weighted avg       0.82      0.91      0.86     30223

Confusion Matrix:
[[27372     1]
 [ 2850     0]]




//output vehicle-loanScript.py

Epoch 1/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 17s 2ms/step - accuracy: 0.7236 - loss: 0.5416 - val_accuracy: 0.8448 - val_loss: 0.2059
Epoch 2/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 13s 2ms/step - accuracy: 0.7834 - loss: 0.4401 - val_accuracy: 0.8684 - val_loss: 0.1848
Epoch 3/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 14s 2ms/step - accuracy: 0.7896 - loss: 0.4307 - val_accuracy: 0.8518 - val_loss: 0.1946
Epoch 4/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.7912 - loss: 0.4278 - val_accuracy: 0.8418 - val_loss: 0.1993
Epoch 5/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 62s 8ms/step - accuracy: 0.7930 - loss: 0.4234 - val_accuracy: 0.8875 - val_loss: 0.1769
Epoch 6/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 16s 2ms/step - accuracy: 0.7928 - loss: 0.4236 - val_accuracy: 0.8753 - val_loss: 0.1853
Epoch 7/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 12s 1ms/step - accuracy: 0.7949 - loss: 0.4216 - val_accuracy: 0.8813 - val_loss: 0.1777
Epoch 8/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 12s 2ms/step - accuracy: 0.7957 - loss: 0.4198 - val_accuracy: 0.8845 - val_loss: 0.1804
Epoch 9/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 12s 1ms/step - accuracy: 0.7975 - loss: 0.4171 - val_accuracy: 0.8584 - val_loss: 0.1925
Epoch 10/10
7951/7951 ━━━━━━━━━━━━━━━━━━━━ 12s 1ms/step - accuracy: 0.7977 - loss: 0.4171 - val_accuracy: 0.8757 - val_loss: 0.1831
1410/1410 ━━━━━━━━━━━━━━━━━━━━ 1s 697us/step 
Classification Report:
              precision    recall  f1-score   support

           0       0.79      0.95      0.86     35178
           1       0.35      0.09      0.14      9921

    accuracy                           0.76     45099
   macro avg       0.57      0.52      0.50     45099
weighted avg       0.69      0.76      0.70     45099

Confusion Matrix:
[[33555  1623]
 [ 9058   863]]
